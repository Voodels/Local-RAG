# ğŸ“– User Guide
## Modular RAG Pipeline v2.0

### Getting Started

Welcome to the Modular RAG Pipeline! This guide will help you get up and running quickly with querying your PDF documents using AI.

---

## ğŸš€ Quick Start

### 1. Installation
```bash
# Clone the repository
git clone <repository-url>
cd rag-pipeline

# Set up virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### 2. Basic Usage
```bash
# Start with your PDF document
python main.py --pdf your_document.pdf --interactive

# Or use our sample document
python main.py --interactive
```

---

## ğŸ¯ Core Concepts

### What is RAG?
**Retrieval Augmented Generation (RAG)** combines the power of information retrieval with language generation:

1. **ğŸ“„ Document Processing**: Your PDF is split into searchable chunks
2. **ğŸ§  Embedding Creation**: Each chunk gets a vector representation
3. **ğŸ” Retrieval**: Find the most relevant chunks for your query
4. **ğŸ’¬ Generation**: AI generates an answer based on relevant context

### How It Works
```
Your Question â†’ Vector Search â†’ Relevant Text â†’ AI Answer
     â†“              â†“              â†“           â†“
"What is X?"  â†’  [0.1, 0.8...]  â†’  "X is..."  â†’  "Based on the document, X is..."
```

---

## ğŸ® Usage Modes

### 1. Interactive Mode
Perfect for exploratory questioning and conversation-like interactions.

```bash
python main.py --interactive
```

**Features:**
- Real-time question answering
- Beautiful terminal interface
- Context-aware responses
- Easy to quit with `exit` or `quit`

**Example Session:**
```
ğŸ’¬ Enter your query: What are the main topics in this document?

ğŸ“Š Search Results
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Score   â”‚ Page â”‚ Content                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0.8945  â”‚  23  â”‚ The document covers machine learning...     â”‚
â”‚ 0.8721  â”‚  45  â”‚ Key topics include neural networks...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Generated Answer
The document covers several main topics including machine learning 
fundamentals, neural network architectures, and practical applications...

ğŸ’¬ Enter your query: Tell me more about neural networks
```

### 2. Single Query Mode
Best for scripting and automation.

```bash
python main.py --query "What is the main conclusion?"
```

**Use Cases:**
- Batch processing
- Automated reports
- Quick fact extraction
- Integration with other tools

### 3. Sample Queries Mode
Great for testing and demonstration.

```bash
python main.py --sample-queries
```

**Includes queries like:**
- "What are the macronutrients?"
- "How do vitamins differ from minerals?"
- "What is the process of digestion?"

---

## âš™ï¸ Configuration

### Command Line Options

#### Basic Options
```bash
# Document settings
--pdf PATH              Path to PDF document
--pdf-url URL          Download PDF from URL

# Model settings  
--model MODEL_ID       HuggingFace model for LLM
--embedding-model NAME Embedding model name
--device {auto,cpu,cuda} Force device usage

# Output settings
--verbose              Enable detailed output
--log-level LEVEL      Set logging level
```

#### Advanced Options
```bash
# Performance tuning
--no-quantization      Disable 4-bit quantization
--embeddings PATH      Custom embeddings file path

# Configuration
--config CONFIG_FILE   Use JSON configuration file
```

### Configuration File
Create a `config.json` file for persistent settings:

```json
{
  "document": {
    "pdf_path": "research_paper.pdf",
    "min_token_length": 30,
    "num_sentence_chunk_size": 10
  },
  "models": {
    "embedding_model_name": "all-mpnet-base-v2",
    "llm_model_id": "microsoft/DialoGPT-medium",
    "use_quantization": true
  },
  "retrieval": {
    "n_resources_to_return": 5,
    "temperature": 0.7,
    "max_new_tokens": 512
  },
  "system": {
    "verbose": true,
    "log_level": "INFO"
  }
}
```

Then use it:
```bash
python main.py --config config.json --interactive
```

### Environment Variables
Set environment variables for system-wide configuration:

```bash
export RAG_PDF_PATH="./documents/default.pdf"
export RAG_LLM_MODEL_ID="microsoft/DialoGPT-medium"
export RAG_VERBOSE="true"
```

---

## ğŸ›ï¸ Model Selection

### Embedding Models

| Model | Size | Speed | Quality | Use Case |
|-------|------|-------|---------|----------|
| `all-MiniLM-L6-v2` | Small | Fast | Good | Quick testing, limited resources |
| `all-mpnet-base-v2` | Medium | Moderate | Excellent | **Recommended default** |
| `all-roberta-large-v1` | Large | Slow | Best | High-quality applications |

```bash
# Fast and lightweight
python main.py --embedding-model all-MiniLM-L6-v2 --interactive

# Best quality (default)
python main.py --embedding-model all-mpnet-base-v2 --interactive

# Highest quality
python main.py --embedding-model all-roberta-large-v1 --interactive
```

### Language Models

| Model | Size | Memory | Quality | Authentication |
|-------|------|--------|---------|----------------|
| `microsoft/DialoGPT-medium` | 1.5B | 4GB | Good | âŒ None |
| `microsoft/DialoGPT-large` | 2.7B | 8GB | Better | âŒ None |
| `google/gemma-2b-it` | 2B | 6GB | Excellent | âœ… Required |

```bash
# Lightweight option
python main.py --model microsoft/DialoGPT-medium --interactive

# Better quality
python main.py --model microsoft/DialoGPT-large --interactive

# Best quality (requires HuggingFace login)
python main.py --model google/gemma-2b-it --interactive
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. CUDA Out of Memory
```
Error: CUDA out of memory
```

**Solutions:**
```bash
# Use CPU instead
python main.py --device cpu --interactive

# Enable quantization (default)
python main.py --interactive

# Use smaller model
python main.py --model microsoft/DialoGPT-medium --interactive
```

#### 2. Model Authentication Required
```
Error: 401 Client Error. Cannot access gated repo
```

**Solution:**
```bash
# Login to HuggingFace
huggingface-cli login

# Or use public models
python main.py --model microsoft/DialoGPT-medium --interactive
```

#### 3. PDF Not Found
```
Error: PDF file not found
```

**Solutions:**
```bash
# Check file path
ls -la your_document.pdf

# Use absolute path
python main.py --pdf /full/path/to/document.pdf --interactive

# Download from URL
python main.py --pdf-url "https://example.com/doc.pdf" --pdf document.pdf --interactive
```

#### 4. spaCy Model Missing
```
Error: Can't find model 'en_core_web_sm'
```

**Solution:**
```bash
python -m spacy download en_core_web_sm
```

### Performance Optimization

#### For Limited Memory
```bash
# Minimal memory configuration
python main.py \
  --model microsoft/DialoGPT-medium \
  --embedding-model all-MiniLM-L6-v2 \
  --device cpu \
  --interactive
```

#### For Best Performance
```bash
# High-performance configuration
python main.py \
  --model microsoft/DialoGPT-large \
  --embedding-model all-mpnet-base-v2 \
  --device cuda \
  --no-quantization \
  --interactive
```

#### For Best Quality
```bash
# Quality-focused configuration (requires auth)
python main.py \
  --model google/gemma-2b-it \
  --embedding-model all-roberta-large-v1 \
  --device cuda \
  --interactive
```

---

## ğŸ“Š Understanding Results

### Search Results Table
```
ğŸ“Š Search Results
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Score   â”‚ Page â”‚ Content                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0.8945  â”‚  23  â”‚ The document covers machine learning...     â”‚
â”‚ 0.8721  â”‚  45  â”‚ Key topics include neural networks...       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Score**: Similarity score (0.0-1.0, higher is better)
- **Page**: Source page number in the PDF
- **Content**: Relevant text chunk from the document

### Quality Indicators

#### High Quality Response
- âœ… High similarity scores (>0.8)
- âœ… Multiple relevant sources
- âœ… Coherent, well-structured answer
- âœ… Cites specific information from the document

#### Lower Quality Response
- âš ï¸ Low similarity scores (<0.6)
- âš ï¸ Generic or vague answer
- âš ï¸ Limited context from document
- âš ï¸ May indicate question not covered in document

---

## ğŸ¯ Best Practices

### Asking Good Questions

#### âœ… Effective Questions
```
â“ "What are the main symptoms of vitamin C deficiency?"
â“ "How does the paper define machine learning?"
â“ "What methodology was used in the study?"
â“ "What are the key findings regarding X?"
```

#### âŒ Less Effective Questions
```
â“ "Tell me everything"  (too broad)
â“ "What do you think?"  (asks for opinion)
â“ "Is this good?"       (subjective)
â“ "What about cats?"    (likely not in document)
```

### Document Preparation

#### âœ… Good Documents
- Clear, well-structured text
- Proper headings and sections
- High-quality PDF (not scanned images)
- Reasonable length (10-1000 pages)

#### âš ï¸ Challenging Documents
- Scanned images without OCR
- Heavy formatting or tables
- Very short documents (<5 pages)
- Extremely long documents (>1000 pages)

### Performance Tips

1. **Start Small**: Test with shorter documents first
2. **Use GPU**: Significantly faster than CPU
3. **Cache Results**: Embeddings are saved and reused
4. **Adjust Settings**: Tune chunk size and retrieval count
5. **Monitor Memory**: Use quantization for large models

---

## ğŸ†˜ Getting Help

### Log Files
Check the log files for detailed error information:
```bash
tail -f rag_pipeline_*.log
```

### Verbose Mode
Enable verbose output for debugging:
```bash
python main.py --verbose --log-level DEBUG --interactive
```

### Community Support
- ğŸ“– **Documentation**: Check our comprehensive docs
- ğŸ› **Issues**: Report bugs on GitHub
- ğŸ’¬ **Discussions**: Join our community forum
- ğŸ“§ **Email**: Contact support team

### System Information
To report issues, include:
```bash
# System info
python --version
pip list | grep torch
nvidia-smi  # if using GPU

# Pipeline info
python main.py --help
```

Happy querying! ğŸš€
